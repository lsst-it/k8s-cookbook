---
crds:
  enabled: false

defaultRules:
  create: true
  labels:
    lsst.io/rule: "true"
  additionalRuleLabels:
    receivers: ",squadcast,"

prometheusOperator:
  enabled: true
  serviceMonitor:
    additionalLabels:
      lsst.io/monitor: "true"
  resources:
    limits:
      cpu: 1
      memory: 128Mi
    requests:
      cpu: 10m
      memory: 128Mi
  prometheusConfigReloader:
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 10m
        memory: 128Mi

prometheus:
  prometheusSpec:
    scrapeInterval: 30s
    resources:
      limits:
        cpu: 4
        memory: 12Gi
      requests:
        cpu: 1
        memory: 12Gi
    externalUrl: "https://prometheus.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
    remoteWrite:
      - url: http://mimir-distributed-gateway.mimir:80/api/v1/push
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    serviceMonitorNamespaceSelector:
      matchLabels:
        lsst.io/discover: "true"
    serviceMonitorSelector:
      matchLabels:
        lsst.io/monitor: "true"
    podMonitorNamespaceSelector:
      matchLabels:
        lsst.io/discover: "true"
    podMonitorSelector:
      matchLabels:
        lsst.io/monitor: "true"
    ruleNamespaceSelector:
      matchLabels:
        lsst.io/discover: "true"
    ruleSelector:
      matchLabels:
        lsst.io/rule: "true"
    scrapeConfigNamespaceSelector:
      matchLabels:
        lsst.io/discover: "true"
    scrapeConfigSelector:
      matchLabels:
        lsst.io/scrape: "true"
    probeNamespaceSelector:
      matchLabels:
        lsst.io/discover: "true"
    probeSelector:
      matchLabels:
        lsst.io/probe: "true"
  ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/backend-protocol: HTTP
      nginx.ingress.kubernetes.io/server-snippet: |
        proxy_ssl_verify off;
    paths:
      - /
    pathType: Prefix
    hosts:
      - "prometheus.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
    tls:
      - secretName: tls-prometheus-ingress
        hosts:
          - "prometheus.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
  serviceMonitor:
    additionalLabels:
      lsst.io/monitor: "true"

alertmanager:
  serviceMonitor:
    additionalLabels:
      lsst.io/monitor: "true"
  alertmanagerSpec:
    resources:
      limits:
        cpu: 1
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 512Mi
    externalUrl: "https://alertmanager.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
    secrets:
      - lsst-webhooks
    configMaps:
      - alertmanager-templates
  ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/backend-protocol: HTTP
      nginx.ingress.kubernetes.io/server-snippet: |
        proxy_ssl_verify off;
    paths:
      - /
    pathType: Prefix
    hosts:
      - "alertmanager.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
    tls:
      - secretName: tls-alertmanager-ingress
        hosts:
          - "alertmanager.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"

grafana:
  serviceMonitor:
    labels:
      lsst.io/monitor: "true"
  resources:
    limits:
      cpu: 4
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 512Mi
  persistence:
    enabled: true
  deploymentStrategy:
    type: Recreate  # default is RollingUpdate, which doesn't work w/ persistence enabled
  grafana.ini:
    server:
      domain: "grafana.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
      root_url: "https://grafana.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
    feature_toggles:
      enable: "autoMigrateOldPanels"
    auth.ldap:
      enabled: true
      allow_sign_up: true
      config_file: /etc/grafana/ldap.toml
    auth.generic_oauth:
      allow_assign_grafana_admin: true
      allow_sign_up: true
      api_url: https://keycloak.ls.lsst.org/realms/master/protocol/openid-connect/userinfo
      auth_url: https://keycloak.ls.lsst.org/realms/master/protocol/openid-connect/auth
      client_id: $__file{/etc/secrets/keycloak-credentials/client_id}
      client_secret: $__file{/etc/secrets/keycloak-credentials/client_secret}
      email_attribute_path: email
      enabled: true
      group_attribute_path: groups
      login_attribute_path: preferred_username
      name: keycloak.ls.lsst.org
      role_attribute_path: contains(groups[*], 'grafana-admin') && 'GrafanaAdmin' || contains(groups[*], 'grafana-admin') && 'Admin' || contains(groups[*], 'grafana-editor') && 'Editor' || 'Viewer'
      scopes: openid profile email groups roles offline_access
      token_url: https://keycloak.ls.lsst.org/realms/master/protocol/openid-connect/token
      use_refresh_token: true
  ldap:
    enabled: true
    existingSecret: grafana-ldap
  ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/backend-protocol: HTTP
      nginx.ingress.kubernetes.io/server-snippet: |
        proxy_ssl_verify off;
    paths:
      - /
    pathType: Prefix
    hosts:
      - "grafana.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
    tls:
      - secretName: tls-grafana-ingress
        hosts:
          - "grafana.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
  sidecar:
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 256Mi
    alerts:
      enabled: true
    dashboards:
      enabled: true
    datasources:
      enabled: true
  extraSecretMounts:
    - name: keycloak-credentials
      mountPath: /etc/secrets/keycloak-credentials
      secretName: grafana-keycloak-credentials
      readOnly: true
  datasources:
    datasource.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          uid: prometheus
          url: http://kube-prometheus-stack-prometheus.kube-prometheus-stack:9090/
          access: proxy
          jsonData:
            httpMethod: POST
            timeInterval: 30s
        - name: Mimir
          type: prometheus
          uid: mimir
          url: http://mimir-distributed-gateway.mimir:80/prometheus
          access: proxy
          isDefault: true
          jsonData:
            httpMethod: POST
            timeInterval: 30s
        - name: Alertmanager
          type: alertmanager
          uid: alertmanager
          url: http://kube-prometheus-stack-alertmanager.kube-prometheus-stack:9093/
          access: proxy
          jsonData:
            handleGrafanaManagedAlerts: false
            implementation: prometheus

# these components are not present in the Rancher clusters
kubeScheduler:
  enabled: false
kubeProxy:
  enabled: false
kubeControllerManager:
  enabled: false

# specify the needed label for the serviceMonitor
kubeApiServer:
  serviceMonitor:
    additionalLabels:
      lsst.io/monitor: "true"
kubelet:
  serviceMonitor:
    additionalLabels:
      lsst.io/monitor: "true"
coreDns:
  serviceMonitor:
    additionalLabels:
      lsst.io/monitor: "true"
kubeDns:
  serviceMonitor:
    additionalLabels:
      lsst.io/monitor: "true"
kubeEtcd:
  serviceMonitor:
    additionalLabels:
      lsst.io/monitor: "true"
prometheus-node-exporter:
  resources:
    limits:
      cpu: 100m
      memory: 64Mi
    requests:
      cpu: 100m
      memory: 64Mi
  prometheus:
    monitor:
      additionalLabels:
        lsst.io/monitor: "true"
kube-state-metrics:
  resources:
    limits:
      cpu: 100m
      memory: 256Mi
    requests:
      cpu: 10m
      memory: 256Mi
  prometheus:
    monitor:
      additionalLabels:
        lsst.io/monitor: "true"
  selfMonitor:
    enabled: true
