---
crds:
  enabled: false

defaultRules:
  create: true
  labels:
    lsst.io/rule: "true"
  additionalRuleLabels:
    receivers: ",squadcast,"

prometheusOperator:
  enabled: true
  serviceMonitor:
    additionalLabels:
      lsst.io/monitor: "true"
  resources:
    limits:
      cpu: 1
      memory: 128Mi
    requests:
      cpu: 10m
      memory: 128Mi
  prometheusConfigReloader:
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 10m
        memory: 128Mi

prometheus:
  prometheusSpec:
    scrapeInterval: 30s
    resources:
      limits:
        cpu: 4
        memory: 12Gi
      requests:
        cpu: 1
        memory: 12Gi
    externalUrl: "https://prometheus.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
    remoteWrite:
      - url: http://mimir-distributed-gateway.mimir:80/api/v1/push
    externalLabels:
      prom: "${ .ClusterLabels.site }/${ .ClusterName }"
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    serviceMonitorNamespaceSelector:
      matchLabels:
        lsst.io/discover: "true"
    serviceMonitorSelector:
      matchLabels:
        lsst.io/monitor: "true"
    podMonitorNamespaceSelector:
      matchLabels:
        lsst.io/discover: "true"
    podMonitorSelector:
      matchLabels:
        lsst.io/monitor: "true"
    ruleNamespaceSelector:
      matchLabels:
        lsst.io/discover: "true"
    ruleSelector:
      matchLabels:
        lsst.io/rule: "true"
    scrapeConfigNamespaceSelector:
      matchLabels:
        lsst.io/discover: "true"
    scrapeConfigSelector:
      matchLabels:
        lsst.io/scrape: "true"
    probeNamespaceSelector:
      matchLabels:
        lsst.io/discover: "true"
    probeSelector:
      matchLabels:
        lsst.io/probe: "true"
  ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/backend-protocol: HTTP
      nginx.ingress.kubernetes.io/server-snippet: |
        proxy_ssl_verify off;
    paths:
      - /
    pathType: Prefix
    hosts:
      - "prometheus.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
    tls:
      - secretName: tls-prometheus-ingress
        hosts:
          - "prometheus.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"

alertmanager:
  serviceMonitor:
    additionalLabels:
      lsst.io/monitor: "true"
  alertmanagerSpec:
    resources:
      limits:
        cpu: 1
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 512Mi
    externalUrl: "https://alertmanager.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
    secrets:
      - lsst-webhooks
    configMaps:
      - alertmanager-templates
  ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/backend-protocol: HTTP
      nginx.ingress.kubernetes.io/server-snippet: |
        proxy_ssl_verify off;
    paths:
      - /
    pathType: Prefix
    hosts:
      - "alertmanager.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
    tls:
      - secretName: tls-alertmanager-ingress
        hosts:
          - "alertmanager.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
  config:
    global:
      resolve_timeout: 5m
      slack_api_url_file: /etc/alertmanager/secrets/lsst-webhooks/slack-test
    route:
      group_by: ["alertname", "namespace"]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 24h
      receiver: "slack-test"
      routes:
        - receiver: "null"
          matchers:
            - alertname = "InfoInhibitor"
        - receiver: "watchdog"
          matchers:
            - alertname = "Watchdog"
        - receiver: "squadcast-test"
          matchers:
            - receivers =~ ".*,squadcast,.*"
          continue: true
        - receiver: slack-kube-test
          matchers:
            - alertname =~ "Kube.*"
        - receiver: slack-node-test
          group_by: ["instance"]
          matchers:
            - alertname =~ "Node.*"
        - receiver: slack-network-test
          group_by: ["instance"]
          matchers:
            - alertname =~ "Network.*"
    receivers:
      - name: "null"
      - name: "watchdog"
      - name: "slack-test"
        slack_configs:
          - username: "${ .ClusterName }-general"
            channel: "#rubinobs-monitoring-test"
            send_resolved: true
            title: '{{ template "slack.o11y.generic.title" . }}'
            text: '{{ template "slack.o11y.generic.text" . }}'
      - name: "slack-kube-test"
        slack_configs:
          - username: "${ .ClusterName }-kube"
            channel: "#rubinobs-monitoring-test"
            send_resolved: true
            title: '{{ template "slack.o11y.kube.title" . }}'
            text: '{{ template "slack.o11y.kube.text" . }}'
      - name: "slack-node-test"
        slack_configs:
          - username: "${ .ClusterName }-nodes"
            channel: "#rubinobs-monitoring-test"
            send_resolved: true
            text: '{{ template "slack.o11y.node.text" . }}'
      - name: "slack-network-test"
        slack_configs:
          - username: "${ .ClusterName }-network"
            channel: "#rubinobs-monitoring-test"
            send_resolved: true
            text: '{{ template "slack.o11y.network.text" . }}'
      - name: "squadcast-test"
        webhook_configs:
          - url_file: /etc/alertmanager/secrets/lsst-webhooks/squadcast-example
    inhibit_rules:
      - source_matchers:
          - alertname = "InfoInhibitor"
        target_matchers:
          - severity = "info"
        equal: ["namespace"]
      - source_matchers:
          - severity = "critical"
        target_matchers:
          - severity =~ "info|warning"
        equal: ["alertname"]
      - source_matchers:
          - severity = "warning"
        target_matchers:
          - severity = "info"
        equal: ["alertname"]
    templates:
      - "/etc/alertmanager/configmaps/alertmanager-templates/*.tmpl"

grafana:
  serviceMonitor:
    labels:
      lsst.io/monitor: "true"
  resources:
    limits:
      cpu: 4
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 512Mi
  persistence:
    enabled: true
  deploymentStrategy:
    type: Recreate  # default is RollingUpdate, which doesn't work w/ persistence enabled
  grafana.ini:
    server:
      domain: "grafana.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
      root_url: "https://grafana.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
    feature_toggles:
      enable: "autoMigrateOldPanels"
    auth.ldap:
      enabled: true
      allow_sign_up: true
      config_file: /etc/grafana/ldap.toml
    auth.generic_oauth:
      allow_assign_grafana_admin: true
      allow_sign_up: true
      api_url: https://keycloak.ls.lsst.org/realms/master/protocol/openid-connect/userinfo
      auth_url: https://keycloak.ls.lsst.org/realms/master/protocol/openid-connect/auth
      client_id: $__file{/etc/secrets/keycloack-credentials/client_id}
      client_secret: $__file{/etc/secrets/keycloack-credentials/client_secret}
      email_attribute_path: email
      enabled: true
      group_attribute_path: groups
      login_attribute_path: preferred_username
      name: keycloack.ls.lsst.org
      role_attribute_path: contains(groups[*], 'grafana-admin') && 'GrafanaAdmin' || contains(groups[*], 'grafana-admin') && 'Admin' || contains(groups[*], 'grafana-editor') && 'Editor' || 'Viewer'
      scopes: openid profile email groups roles offline_access
      token_url: https://keycloak.ls.lsst.org/realms/master/protocol/openid-connect/token
      use_refresh_token: true
  ldap:
    enabled: true
    existingSecret: grafana-ldap
  ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/backend-protocol: HTTP
      nginx.ingress.kubernetes.io/server-snippet: |
        proxy_ssl_verify off;
    paths:
      - /
    pathType: Prefix
    hosts:
      - "grafana.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
    tls:
      - secretName: tls-grafana-ingress
        hosts:
          - "grafana.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org"
  sidecar:
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 256Mi
    alerts:
      enabled: true
    dashboards:
      enabled: true
  datasources:
    datasource.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          uid: prometheus
          url: http://kube-prometheus-stack-prometheus.kube-prometheus-stack:9090/
          access: proxy
          jsonData:
            httpMethod: POST
            timeInterval: 30s
        - name: Mimir
          type: prometheus
          uid: mimir
          url: http://mimir-distributed-gateway.mimir:80/prometheus
          access: proxy
          isDefault: true
          jsonData:
            httpMethod: POST
            timeInterval: 30s
        - name: Alertmanager
          type: alertmanager
          uid: alertmanager
          url: http://kube-prometheus-stack-alertmanager.kube-prometheus-stack:9093/
          access: proxy
          jsonData:
            handleGrafanaManagedAlerts: false
            implementation: prometheus

# these components are not present in the Rancher clusters
kubeScheduler:
  enabled: false
kubeProxy:
  enabled: false
kubeControllerManager:
  enabled: false

# specify the needed label for the serviceMonitor
kubelet:
  serviceMonitor:
    additionalLabels:
      lsst.io/monitor: "true"
coreDns:
  serviceMonitor:
    additionalLabels:
      lsst.io/monitor: "true"
kubeDns:
  serviceMonitor:
    additionalLabels:
      lsst.io/monitor: "true"
kubeEtcd:
  serviceMonitor:
    additionalLabels:
      lsst.io/monitor: "true"
prometheus-node-exporter:
  resources:
    limits:
      cpu: 100m
      memory: 64Mi
    requests:
      cpu: 100m
      memory: 64Mi
  prometheus:
    monitor:
      additionalLabels:
        lsst.io/monitor: "true"
kube-state-metrics:
  resources:
    limits:
      cpu: 100m
      memory: 256Mi
    requests:
      cpu: 10m
      memory: 256Mi
  prometheus:
    monitor:
      additionalLabels:
        lsst.io/monitor: "true"
  selfMonitor:
    enabled: true
