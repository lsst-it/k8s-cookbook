---
operatorNamespace: rook-ceph

toolbox:
  enabled: true
  tolerations:
    - key: node.kubernetes.io/unreachable
      operator: Exists
      effect: NoExecute
      tolerationSeconds: 5
    - key: role
      operator: Equal
      value: storage-node
      effect: NoSchedule
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: role
                operator: In
                values:
                  - storage-node

monitoring:
  enabled: true
  createPrometheusRules: true
  rulesNamespaceOverride: rook-ceph
  prometheusRule:
    labels:
      lsst.io/rule: "true"

cephClusterSpec:
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.4
    allowUnsupported: false
  labels:
    monitoring:
      lsst.io/monitor: "true"
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  waitTimeoutForHealthyOSDInMinutes: 10
  mon:
    allowMultiplePerNode: false
  mgr:
    count: 2
    modules:
      - name: pg_autoscaler
        enabled: true
  dashboard:
    enabled: true
    ssl: false
  crashCollector:
    disable: false
  logCollector:
    enabled: true
    periodicity: 1d  # SUFFIX may be 'h' for hours or 'd' for days.
  cleanupPolicy:
    #confirmation: "yes-really-destroy-data"
    sanitizeDisks:
      method: quick
      dataSource: zero
      iteration: 1
    allowUninstallWithVolumes: false
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: role
                  operator: In
                  values:
                    - storage-node
      tolerations:
        - key: role
          operator: Equal
          value: storage-node
          effect: NoSchedule
  removeOSDsIfOutAndSafeToRemove: false
  #  priorityClassNames:
  #    all: rook-ceph-default-priority-class
  #    mon: rook-ceph-mon-priority-class
  #    osd: rook-ceph-osd-priority-class
  #    mgr: rook-ceph-mgr-priority-class
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 30
    manageMachineDisruptionBudgets: false
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    livenessProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false
  resources:
    mgr:
      limits:
        cpu: "1"
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 512Mi
    mon:
      limits:
        cpu: "2"
        memory: 2Gi
      requests:
        cpu: "1"
        memory: 1Gi
    osd:
      limits:
        cpu: "2"
        memory: 12Gi
      requests:
        cpu: 500m
        memory: 8Gi
    prepareosd:
      limits:
        cpu: 500m
        memory: 32Gi
      requests:
        cpu: 500m
        memory: 50Mi
    mgr-sidecar:
      limits:
        cpu: 500m
        memory: 100Mi
      requests:
        cpu: 100m
        memory: 40Mi
    crashcollector:
      limits:
        cpu: 500m
        memory: 60Mi
      requests:
        cpu: 100m
        memory: 60Mi
    logcollector:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 100Mi
    cleanup:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 100Mi

ingress:
  dashboard:
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
    host:
      name: &hostname ceph.${ .ClusterName }.${ .ClusterLabels.site }.lsst.org
    ingressClassName: nginx
    tls:
      - hosts:
          - *hostname
        secretName: rook-ceph-mgr-dashboard-ingress-tls

cephBlockPools: {}
cephFileSystems: {}
cephFileSystemVolumeSnapshotClass: {}
cephBlockPoolsVolumeSnapshotClass: {}
cephObjectStores: {}
