---
# XXX to allow access to the s3 creds. Can this be more restrictively scoped?
global:
  extraEnvFrom:
    - secretRef:
        name: mimir-s3

metaMonitoring:
  dashboards:
    enabled: true
    labels:
      lsst.io/dashboard: "true"
      grafana_dashboard: "1"
  serviceMonitor:
    enabled: true
    labels:
      lsst.io/monitor: "true"
  prometheusRule:
    enabled: true
    labels:
      lsst.io/rule: "true"
mimir:
  structuredConfig:
    common:
      storage:
        backend: s3
        s3:
          region: o11y
          bucket_name: ${ get .ClusterLabels "management.cattle.io/cluster-display-name" }-mimir
          endpoint: s3.o11y.${ get .ClusterLabels "management.cattle.io/cluster-display-name" }.${ .ClusterLabels.site }.lsst.org
          access_key_id: ${`${AWS_ACCESS_KEY_ID}`}
          secret_access_key: ${`${AWS_SECRET_ACCESS_KEY}`}
    blocks_storage:
      s3:
        bucket_name: ${ get .ClusterLabels "management.cattle.io/cluster-display-name" }-mimir-blocks
    alertmanager_storage:
      s3:
        bucket_name: ${ get .ClusterLabels "management.cattle.io/cluster-display-name" }-mimir-alertmanager
    ruler_storage:
      s3:
        bucket_name: ${ get .ClusterLabels "management.cattle.io/cluster-display-name" }-mimir-ruler
    limits:
      out_of_order_time_window: 30s
      ingestion_rate: 1000000
      ingestion_burst_size: 2000000
      max_global_series_per_user: 0
      max_global_series_per_metric: 0
      compactor_blocks_retention_period: 2y
    compactor:
      deletion_delay: 12h

compactor:
  persistentVolume:
    size: 100Gi
  resources:
    limits:
      cpu: 1
      memory: 4Gi
    requests:
      cpu: 1
      memory: 4Gi
distributor:
  replicas: 2
  resources:
    limits:
      cpu: 12
      memory: 12Gi
    requests:
      cpu: 1
      memory: 12Gi
ingester:
  persistentVolume:
    size: 200Gi
  replicas: 3
  resources:
    limits:
      cpu: 18
      memory: 48Gi
    requests:
      cpu: 5
      memory: 48Gi
  topologySpreadConstraints: {}
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: target  # support for enterprise.legacyLabels
                operator: In
                values:
                  - ingester
          topologyKey: kubernetes.io/hostname

        - labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                  - ingester
          topologyKey: kubernetes.io/hostname

  zoneAwareReplication:
    topologyKey: kubernetes.io/hostname
querier:
  replicas: 2
  resources:
    limits:
      cpu: 10
      memory: 10Gi
    requests:
      cpu: 2
      memory: 10Gi
query_frontend:
  replicas: 2
  resources:
    limits:
      cpu: 2
      memory: 3Gi
    requests:
      cpu: 2
      memory: 3Gi
query_scheduler:
  replicas: 2
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 1Gi
ruler:
  replicas: 2
  resources:
    limits:
      cpu: 1
      memory: 3Gi
    requests:
      cpu: 1
      memory: 3Gi
store_gateway:
  persistentVolume:
    size: 100Gi
  replicas: 3
  resources:
    limits:
      cpu: 10
      memory: 10Gi
    requests:
      cpu: 1
      memory: 10Gi
  topologySpreadConstraints: {}
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: target  # support for enterprise.legacyLabels
                operator: In
                values:
                  - store-gateway
          topologyKey: kubernetes.io/hostname

        - labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                  - store-gateway
          topologyKey: kubernetes.io/hostname
  zoneAwareReplication:
    topologyKey: kubernetes.io/hostname

## caching components
admin-cache:
  enabled: true
  replicas: 2
chunks-cache:
  enabled: true
  replicas: 2
index-cache:
  enabled: true
  replicas: 3
metadata-cache:
  enabled: true
results-cache:
  enabled: true
  replicas: 2

## exporter
overrides_exporter:
  replicas: 1
  resources:
    limits:
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi

# NGINX deperecated, use gateway instead
nginx:
  enabled: false
gateway:
  enabledNonEnterprise: true
  replicas: 2
  nginx:
    config:
      clientMaxBodySize: 10m
      serverSnippet: |
        client_body_buffer_size 10m;
  ingress:
    enabled: true
    nameOverride: mimir-nginx
    ingressClassName: nginx
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
      nginx.ingress.kubernetes.io/client-body-buffer-size: 10m
      nginx.ingress.kubernetes.io/proxy-body-size: 10m
      nginx.ingress.kubernetes.io/backend-protocol: HTTP
    hosts:
      - host: mimir.${ get .ClusterLabels "management.cattle.io/cluster-display-name" }.${ .ClusterLabels.site }.lsst.org
        paths:
          - path: /
            pathType: Prefix
    tls:
      - secretName: tls-mimir-ingress
        hosts:
          - mimir.${ get .ClusterLabels "management.cattle.io/cluster-display-name" }.${ .ClusterLabels.site }.lsst.org
  resources:
    limits:
      memory: 1250Mi
    requests:
      cpu: 1500m
      memory: 1250Mi

# No use for minio, using the rook storage setup
minio:
  enabled: false

# Disable the alertmanager
alertmanager:
  enabled: false

# Grafana Enterprise Metrics feature related
admin_api:
  enabled: false
enterprise:
  enabled: false
