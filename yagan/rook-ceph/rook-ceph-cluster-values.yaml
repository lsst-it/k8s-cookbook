---
operatorNamespace: rook-ceph

toolbox:
  enabled: true
  tolerations:
    - key: node.kubernetes.io/unreachable
      operator: Exists
      effect: NoExecute
      tolerationSeconds: 5
    - key: role
      operator: Equal
      value: storage-node
      effect: NoSchedule
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: role
                operator: In
                values:
                  - storage-node

monitoring:
  enabled: true
  rulesNamespaceOverride: rook-ceph

cephClusterSpec:
  cephVersion:
    image: quay.io/ceph/ceph:v17.2.6
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  waitTimeoutForHealthyOSDInMinutes: 10
  mon:
    count: 5
    allowMultiplePerNode: false
  mgr:
    count: 2
    modules:
      - name: pg_autoscaler
        enabled: true
  dashboard:
    enabled: true
    ssl: true
  crashCollector:
    disable: false
  logCollector:
    enabled: true
    periodicity: 1d  # SUFFIX may be 'h' for hours or 'd' for days.
  cleanupPolicy:
    #confirmation: "yes-really-destroy-data"
    sanitizeDisks:
      method: quick
      dataSource: zero
      iteration: 1
    allowUninstallWithVolumes: false
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: role
                  operator: In
                  values:
                    - storage-node
      tolerations:
        - key: role
          operator: Equal
          value: storage-node
          effect: NoSchedule
  removeOSDsIfOutAndSafeToRemove: false
  #  priorityClassNames:
  #    all: rook-ceph-default-priority-class
  #    mon: rook-ceph-mon-priority-class
  #    osd: rook-ceph-osd-priority-class
  #    mgr: rook-ceph-mgr-priority-class
  storage:
    useAllNodes: false
    useAllDevices: false
    config:
      osdsPerDevice: "4"
    nodes:
      - name: yagan01
        devices:
          - name: /dev/disk/by-id/nvme-SAMSUNG_MZQLB1T9HAJR-00007_S439NC0R703118
          - name: /dev/disk/by-id/nvme-SAMSUNG_MZQLB1T9HAJR-00007_S439NC0R703123
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX2202062X3P8CGN
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX220206NF3P8CGN
      - name: yagan02
        devices:
          - name: /dev/disk/by-id/nvme-SAMSUNG_MZQLB1T9HAJR-00007_S439NC0R703121
          - name: /dev/disk/by-id/nvme-SAMSUNG_MZQLB1T9HAJR-00007_S439NC0R703128
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX220202T43P8CGN
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX2202043E3P8CGN
      - name: yagan03
        devices:
          - name: /dev/disk/by-id/nvme-SAMSUNG_MZQLB1T9HAJR-00007_S439NC0R605075
          - name: /dev/disk/by-id/nvme-SAMSUNG_MZQLB1T9HAJR-00007_S439NC0R703127
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX2202041Z3P8CGN
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX2202043D3P8CGN
      - name: yagan04
        devices:
          - name: /dev/disk/by-id/nvme-SAMSUNG_MZQLB1T9HAJR-00007_S439NC0R703122
          - name: /dev/disk/by-id/nvme-SAMSUNG_MZQLB1T9HAJR-00007_S439NC0R703125
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX220102ER3P8CGN
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX220202S53P8CGN
      - name: yagan05
        devices:
          - name: /dev/disk/by-id/nvme-SAMSUNG_MZQLB1T9HAJR-00007_S439NC0R703108
          - name: /dev/disk/by-id/nvme-SAMSUNG_MZQLB1T9HAJR-00007_S439NC0R703130
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX220102HH3P8CGN
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX220202S63P8CGN
      - name: yagan06
        devices:
          - name: /dev/disk/by-id/nvme-SAMSUNG_MZQLB1T9HAJR-00007_S439NC0R703106
          - name: /dev/disk/by-id/nvme-SAMSUNG_MZQLB1T9HAJR-00007_S439NC0R703131
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX220102FE3P8CGN
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX220102GL3P8CGN
      - name: yagan07
        devices:
          - name: /dev/disk/by-id/nvme-SAMSUNG_MZQLB1T9HAJR-00007_S439NC0R605071
          - name: /dev/disk/by-id/nvme-SAMSUNG_MZQLB1T9HAJR-00007_S439NC0R703134
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX220102BC3P8CGN
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX220102KZ3P8CGN
      - name: yagan08
        devices:
          - name: /dev/disk/by-id/nvme-MTFDHAL1T9TCT_190620D56A01
          - name: /dev/disk/by-id/nvme-MTFDHAL1T9TCT_190620D57152
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX2201029T3P8CGN
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX220103XG3P8CGN
      - name: yagan09
        devices:
          - name: /dev/disk/by-id/nvme-MTFDHAL1T9TCT_190620D56B58
          - name: /dev/disk/by-id/nvme-MTFDHAL1T9TCT_190620D56DDC
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX220102BR3P8CGN
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX220103XE3P8CGN
      - name: yagan10
        devices:
          - name: /dev/disk/by-id/nvme-MTFDHAL1T9TCT_190120D61699
          - name: /dev/disk/by-id/nvme-MTFDHAL1T9TCT_190620D56DE7
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX220102JB3P8CGN
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX220206313P8CGN
      - name: yagan11
        config:
          osdsPerDevice: "4"
        devices:
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX236402R53P8CGN
      - name: yagan12
        config:
          osdsPerDevice: "1"
        devices:
          - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX235301FJ3P8CGN
      #- name: yagan13
      #  devices:
      #    - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX236402JT3P8CGN
      #- name: yagan14
      #  devices:
      #    - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX235300HB3P8CGN
      #- name: yagan15
      #  devices:
      #    - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX236402NF3P8CGN
      #- name: yagan16
      #  devices:
      #    - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX236402ME3P8CGN
      #- name: yagan17
      #  devices:
      #    - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX236400K43P8CGN
      #- name: yagan18
      #  devices:
      #    - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX235300GA3P8CGN
      #- name: yagan19
      #  devices:
      #    - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX236302WV3P8CGN
      #- name: yagan20
      #  devices:
      #    - name: /dev/disk/by-id/nvme-INTEL_SSDPF2KX038T1_PHAX236302LQ3P8CGN
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 30
    manageMachineDisruptionBudgets: false
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    livenessProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false
  resources:
    mgr:
      limits:
        cpu: "1000m"
        memory: "1Gi"
      requests:
        cpu: "500m"
        memory: "512Mi"
    mon:
      limits:
        cpu: "2000m"
        memory: "2Gi"
      requests:
        cpu: "1000m"
        memory: "1Gi"
    osd:
      limits:
        cpu: "2000m"
        memory: "8Gi"
      requests:
        cpu: "1000m"
        memory: "8Gi"
    prepareosd:
      limits:
        cpu: "500m"
        memory: "400Mi"
      requests:
        cpu: "500m"
        memory: "50Mi"
    mgr-sidecar:
      limits:
        cpu: "500m"
        memory: "100Mi"
      requests:
        cpu: "100m"
        memory: "40Mi"
    crashcollector:
      limits:
        cpu: "500m"
        memory: "60Mi"
      requests:
        cpu: "100m"
        memory: "60Mi"
    logcollector:
      limits:
        cpu: "500m"
        memory: "1Gi"
      requests:
        cpu: "100m"
        memory: "100Mi"
    cleanup:
      limits:
        cpu: "500m"
        memory: "1Gi"
      requests:
        cpu: "500m"
        memory: "100Mi"

ingress:
  dashboard:
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
      nginx.ingress.kubernetes.io/server-snippet: |
        proxy_ssl_verify off;
    host:
      name: &hostname ceph.yagan.cp.lsst.org
    tls:
      - hosts:
          - *hostname
        secretName: rook-ceph-mgr-dashboard-ingress-tls

cephBlockPools:
cephFileSystems:
cephFileSystemVolumeSnapshotClass:
cephBlockPoolsVolumeSnapshotClass:
cephObjectStores:
